{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:07:30.697809Z",
     "start_time": "2025-03-01T17:07:02.863326Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59981caa5b06a9a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:12:30.730566Z",
     "start_time": "2025-03-01T17:12:30.708325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>15.689439</td>\n",
       "      <td>15.753504</td>\n",
       "      <td>15.621622</td>\n",
       "      <td>15.684434</td>\n",
       "      <td>15.684434</td>\n",
       "      <td>78169752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>15.695195</td>\n",
       "      <td>15.711712</td>\n",
       "      <td>15.554054</td>\n",
       "      <td>15.615365</td>\n",
       "      <td>15.615365</td>\n",
       "      <td>120067812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>15.662162</td>\n",
       "      <td>15.662162</td>\n",
       "      <td>15.174174</td>\n",
       "      <td>15.221722</td>\n",
       "      <td>15.221722</td>\n",
       "      <td>158988852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>15.250250</td>\n",
       "      <td>15.265265</td>\n",
       "      <td>14.831081</td>\n",
       "      <td>14.867367</td>\n",
       "      <td>14.867367</td>\n",
       "      <td>256315428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>14.814815</td>\n",
       "      <td>15.096346</td>\n",
       "      <td>14.742492</td>\n",
       "      <td>15.065566</td>\n",
       "      <td>15.065566</td>\n",
       "      <td>188783028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>87.110001</td>\n",
       "      <td>89.550003</td>\n",
       "      <td>87.070000</td>\n",
       "      <td>89.230003</td>\n",
       "      <td>89.230003</td>\n",
       "      <td>23003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>88.800003</td>\n",
       "      <td>88.940002</td>\n",
       "      <td>87.010002</td>\n",
       "      <td>87.389999</td>\n",
       "      <td>87.389999</td>\n",
       "      <td>20097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>86.980003</td>\n",
       "      <td>88.040001</td>\n",
       "      <td>85.940002</td>\n",
       "      <td>86.019997</td>\n",
       "      <td>86.019997</td>\n",
       "      <td>19523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>86.620003</td>\n",
       "      <td>88.849998</td>\n",
       "      <td>86.610001</td>\n",
       "      <td>88.449997</td>\n",
       "      <td>88.449997</td>\n",
       "      <td>23333500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>86.980003</td>\n",
       "      <td>88.300003</td>\n",
       "      <td>86.570000</td>\n",
       "      <td>88.230003</td>\n",
       "      <td>88.230003</td>\n",
       "      <td>23986300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3272 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2010-01-04  15.689439  15.753504  15.621622  15.684434  15.684434   \n",
       "1     2010-01-05  15.695195  15.711712  15.554054  15.615365  15.615365   \n",
       "2     2010-01-06  15.662162  15.662162  15.174174  15.221722  15.221722   \n",
       "3     2010-01-07  15.250250  15.265265  14.831081  14.867367  14.867367   \n",
       "4     2010-01-08  14.814815  15.096346  14.742492  15.065566  15.065566   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "3267  2022-12-23  87.110001  89.550003  87.070000  89.230003  89.230003   \n",
       "3268  2022-12-27  88.800003  88.940002  87.010002  87.389999  87.389999   \n",
       "3269  2022-12-28  86.980003  88.040001  85.940002  86.019997  86.019997   \n",
       "3270  2022-12-29  86.620003  88.849998  86.610001  88.449997  88.449997   \n",
       "3271  2022-12-30  86.980003  88.300003  86.570000  88.230003  88.230003   \n",
       "\n",
       "         Volume  \n",
       "0      78169752  \n",
       "1     120067812  \n",
       "2     158988852  \n",
       "3     256315428  \n",
       "4     188783028  \n",
       "...         ...  \n",
       "3267   23003000  \n",
       "3268   20097300  \n",
       "3269   19523200  \n",
       "3270   23333500  \n",
       "3271   23986300  \n",
       "\n",
       "[3272 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('dataset/Google_Stock_Train.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d7f3953a78ee693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:12:33.892273Z",
     "start_time": "2025-03-01T17:12:33.849843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>89.589996</td>\n",
       "      <td>91.050003</td>\n",
       "      <td>88.519997</td>\n",
       "      <td>89.120003</td>\n",
       "      <td>89.120003</td>\n",
       "      <td>28131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>90.349998</td>\n",
       "      <td>90.650002</td>\n",
       "      <td>87.269997</td>\n",
       "      <td>88.080002</td>\n",
       "      <td>88.080002</td>\n",
       "      <td>34854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>87.470001</td>\n",
       "      <td>87.570000</td>\n",
       "      <td>85.900002</td>\n",
       "      <td>86.199997</td>\n",
       "      <td>86.199997</td>\n",
       "      <td>27194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>86.790001</td>\n",
       "      <td>87.690002</td>\n",
       "      <td>84.860001</td>\n",
       "      <td>87.339996</td>\n",
       "      <td>87.339996</td>\n",
       "      <td>41381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>88.360001</td>\n",
       "      <td>90.050003</td>\n",
       "      <td>87.860001</td>\n",
       "      <td>88.019997</td>\n",
       "      <td>88.019997</td>\n",
       "      <td>29003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>121.660004</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>120.980003</td>\n",
       "      <td>121.529999</td>\n",
       "      <td>121.529999</td>\n",
       "      <td>29686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>121.360001</td>\n",
       "      <td>123.150002</td>\n",
       "      <td>121.019997</td>\n",
       "      <td>122.209999</td>\n",
       "      <td>122.209999</td>\n",
       "      <td>52509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>130.070007</td>\n",
       "      <td>130.979996</td>\n",
       "      <td>128.320007</td>\n",
       "      <td>129.270004</td>\n",
       "      <td>129.270004</td>\n",
       "      <td>61682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2023-07-27</td>\n",
       "      <td>131.669998</td>\n",
       "      <td>133.240005</td>\n",
       "      <td>128.789993</td>\n",
       "      <td>129.399994</td>\n",
       "      <td>129.399994</td>\n",
       "      <td>44952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>130.779999</td>\n",
       "      <td>133.740005</td>\n",
       "      <td>130.570007</td>\n",
       "      <td>132.580002</td>\n",
       "      <td>132.580002</td>\n",
       "      <td>36572900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2023-01-03   89.589996   91.050003   88.519997   89.120003   89.120003   \n",
       "1    2023-01-04   90.349998   90.650002   87.269997   88.080002   88.080002   \n",
       "2    2023-01-05   87.470001   87.570000   85.900002   86.199997   86.199997   \n",
       "3    2023-01-06   86.790001   87.690002   84.860001   87.339996   87.339996   \n",
       "4    2023-01-09   88.360001   90.050003   87.860001   88.019997   88.019997   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "138  2023-07-24  121.660004  123.000000  120.980003  121.529999  121.529999   \n",
       "139  2023-07-25  121.360001  123.150002  121.019997  122.209999  122.209999   \n",
       "140  2023-07-26  130.070007  130.979996  128.320007  129.270004  129.270004   \n",
       "141  2023-07-27  131.669998  133.240005  128.789993  129.399994  129.399994   \n",
       "142  2023-07-28  130.779999  133.740005  130.570007  132.580002  132.580002   \n",
       "\n",
       "       Volume  \n",
       "0    28131200  \n",
       "1    34854800  \n",
       "2    27194400  \n",
       "3    41381500  \n",
       "4    29003900  \n",
       "..        ...  \n",
       "138  29686100  \n",
       "139  52509600  \n",
       "140  61682100  \n",
       "141  44952100  \n",
       "142  36572900  \n",
       "\n",
       "[143 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('dataset/Google_Stock_Test.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d1efe01cfe1d995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:16:09.060589Z",
     "start_time": "2025-03-01T17:16:09.047263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>15.689439</td>\n",
       "      <td>15.753504</td>\n",
       "      <td>15.621622</td>\n",
       "      <td>15.684434</td>\n",
       "      <td>15.684434</td>\n",
       "      <td>78169752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>15.695195</td>\n",
       "      <td>15.711712</td>\n",
       "      <td>15.554054</td>\n",
       "      <td>15.615365</td>\n",
       "      <td>15.615365</td>\n",
       "      <td>120067812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>15.662162</td>\n",
       "      <td>15.662162</td>\n",
       "      <td>15.174174</td>\n",
       "      <td>15.221722</td>\n",
       "      <td>15.221722</td>\n",
       "      <td>158988852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>15.250250</td>\n",
       "      <td>15.265265</td>\n",
       "      <td>14.831081</td>\n",
       "      <td>14.867367</td>\n",
       "      <td>14.867367</td>\n",
       "      <td>256315428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>14.814815</td>\n",
       "      <td>15.096346</td>\n",
       "      <td>14.742492</td>\n",
       "      <td>15.065566</td>\n",
       "      <td>15.065566</td>\n",
       "      <td>188783028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>121.660004</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>120.980003</td>\n",
       "      <td>121.529999</td>\n",
       "      <td>121.529999</td>\n",
       "      <td>29686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>121.360001</td>\n",
       "      <td>123.150002</td>\n",
       "      <td>121.019997</td>\n",
       "      <td>122.209999</td>\n",
       "      <td>122.209999</td>\n",
       "      <td>52509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>130.070007</td>\n",
       "      <td>130.979996</td>\n",
       "      <td>128.320007</td>\n",
       "      <td>129.270004</td>\n",
       "      <td>129.270004</td>\n",
       "      <td>61682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413</th>\n",
       "      <td>2023-07-27</td>\n",
       "      <td>131.669998</td>\n",
       "      <td>133.240005</td>\n",
       "      <td>128.789993</td>\n",
       "      <td>129.399994</td>\n",
       "      <td>129.399994</td>\n",
       "      <td>44952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3414</th>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>130.779999</td>\n",
       "      <td>133.740005</td>\n",
       "      <td>130.570007</td>\n",
       "      <td>132.580002</td>\n",
       "      <td>132.580002</td>\n",
       "      <td>36572900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3415 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "0     2010-01-04   15.689439   15.753504   15.621622   15.684434   15.684434   \n",
       "1     2010-01-05   15.695195   15.711712   15.554054   15.615365   15.615365   \n",
       "2     2010-01-06   15.662162   15.662162   15.174174   15.221722   15.221722   \n",
       "3     2010-01-07   15.250250   15.265265   14.831081   14.867367   14.867367   \n",
       "4     2010-01-08   14.814815   15.096346   14.742492   15.065566   15.065566   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "3410  2023-07-24  121.660004  123.000000  120.980003  121.529999  121.529999   \n",
       "3411  2023-07-25  121.360001  123.150002  121.019997  122.209999  122.209999   \n",
       "3412  2023-07-26  130.070007  130.979996  128.320007  129.270004  129.270004   \n",
       "3413  2023-07-27  131.669998  133.240005  128.789993  129.399994  129.399994   \n",
       "3414  2023-07-28  130.779999  133.740005  130.570007  132.580002  132.580002   \n",
       "\n",
       "         Volume  \n",
       "0      78169752  \n",
       "1     120067812  \n",
       "2     158988852  \n",
       "3     256315428  \n",
       "4     188783028  \n",
       "...         ...  \n",
       "3410   29686100  \n",
       "3411   52509600  \n",
       "3412   61682100  \n",
       "3413   44952100  \n",
       "3414   36572900  \n",
       "\n",
       "[3415 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4886d174ad6fb21b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:12:37.375599Z",
     "start_time": "2025-03-01T17:12:37.263949Z"
    }
   },
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler().fit(full_data.iloc[:, 4:5].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253ffaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.031017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.029893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>0.563735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>0.550490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>0.540629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>0.558120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>0.556537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3272 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0     0.034348\n",
       "1     0.033850\n",
       "2     0.031017\n",
       "3     0.028466\n",
       "4     0.029893\n",
       "...        ...\n",
       "3267  0.563735\n",
       "3268  0.550490\n",
       "3269  0.540629\n",
       "3270  0.558120\n",
       "3271  0.556537\n",
       "\n",
       "[3272 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = minmax.transform(train_data.iloc[:, 4:5]).astype('float32')\n",
    "train_data = pd.DataFrame(train_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14662f3102019ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:14:08.686624Z",
     "start_time": "2025-03-01T17:14:08.658140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.562943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.541925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.550130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.555025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.796233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.801128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.851946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.852882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.875772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0    0.562943\n",
       "1    0.555457\n",
       "2    0.541925\n",
       "3    0.550130\n",
       "4    0.555025\n",
       "..        ...\n",
       "138  0.796233\n",
       "139  0.801128\n",
       "140  0.851946\n",
       "141  0.852882\n",
       "142  0.875772\n",
       "\n",
       "[143 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = minmax.transform(test_data.iloc[:, 4:5]).astype('float32')\n",
    "test_data = pd.DataFrame(test_data)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562a29c",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66d96e59706fd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model):\n",
    "    modelnn = model\n",
    "    date_ori = pd.to_datetime(full_data.iloc[:, 0]).tolist()\n",
    "\n",
    "    pbar = tqdm(range(epoch), desc = 'train loop')\n",
    "    for i in pbar:\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, train_data.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, train_data.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                train_data.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = train_data.iloc[k + 1 : index + 1, :].values\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = modelnn(batch_x)\n",
    "                loss = modelnn.compute_loss(batch_y, logits)\n",
    "            \n",
    "            gradients = tape.gradient(loss, modelnn.trainable_variables)\n",
    "            modelnn.optimizer.apply_gradients(zip(gradients, modelnn.trainable_variables))\n",
    "            \n",
    "            total_loss.append(loss.numpy())\n",
    "            total_acc.append(modelnn.compute_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "        pbar.set_postfix(cost = np.mean(total_loss), acc = np.mean(total_acc))\n",
    "\n",
    "    future_day = test_size\n",
    "    output_predict = np.zeros((full_data_train.shape[0] + future_day, full_data_train.shape[1]))\n",
    "    output_predict[0] = full_data_train.iloc[0]\n",
    "\n",
    "    for k in range(0, full_data_train.shape[0], timestamp):\n",
    "        index = min(k + timestamp, full_data_train.shape[0])\n",
    "        batch_x = np.expand_dims(full_data_train.iloc[k:index, :].values, axis=0)\n",
    "        out_logits = modelnn(batch_x).numpy()\n",
    "        output_predict[k + 1:index + 1] = out_logits\n",
    "\n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        batch_x = np.expand_dims(o, axis=0)\n",
    "        out_logits = modelnn(batch_x).numpy()\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "        date_ori.append(date_ori[-1] + timedelta(days=1))\n",
    "\n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "\n",
    "    return deep_future[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_accuracy(results):\n",
    "    accuracies = [calculate_accuracy(full_data['Close'].iloc[-test_size:].values, r) for r in results]\n",
    "\n",
    "    plt.figure(figsize = (15, 5))\n",
    "    for no, r in enumerate(results):\n",
    "        plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    plt.plot(full_data['Close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "    plt.legend()\n",
    "    plt.title('average accuracy: %.4f'%(np.mean(accuracies)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa751ba1acf1a6e1",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f58152bb41dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        super(LstmModel, self).__init__()\n",
    "        \n",
    "        self.lstm_layers = [\n",
    "            tf.keras.layers.LSTM(\n",
    "                size_layer,\n",
    "                return_sequences=True,\n",
    "                dropout=forget_bias,\n",
    "            ) for _ in range(num_layers)\n",
    "        ]\n",
    "    \n",
    "        self.dense = tf.keras.layers.Dense(output_size)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            x = lstm_layer(x)\n",
    "        \n",
    "        logits = self.dense(x[:, -1, :])\n",
    "        return logits\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        real = np.array(y_true) + 1\n",
    "        predict = np.array(y_pred) + 1\n",
    "        percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "        return percentage * 100\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b43fede439b6bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 300\n",
    "dropout_rate = 0.8\n",
    "future_day = test_data.shape[0]\n",
    "learning_rate = 0.01\n",
    "simulation_size = 10\n",
    "test_size = test_data.shape[0]\n",
    "\n",
    "lstm = LstmModel(\n",
    "    learning_rate, \n",
    "    num_layers, \n",
    "    full_data.shape[1], \n",
    "    size_layer, \n",
    "    full_data.shape[1], \n",
    "    dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71164aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 144/300 [46:03<1:08:04, 26.18s/it, acc=99.1, cost=0.000531]"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    results.append(forecast(lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_accuracy(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff18bd",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edaba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLstmModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        super(BiLstmModel, self).__init__()\n",
    "        \n",
    "        self.lstm_layers = [\n",
    "            tf.keras.layers.LSTM(\n",
    "                size_layer,\n",
    "                return_sequences=True,\n",
    "                dropout=forget_bias,\n",
    "            ) for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.bidirectional_lstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(size_layer, return_sequences=True)\n",
    "        )\n",
    "    \n",
    "        self.dense = tf.keras.layers.Dense(output_size)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            x = lstm_layer(x)\n",
    "\n",
    "        x = self.bidirectional_lstm(x)\n",
    "        \n",
    "        logits = self.dense(x[:, -1, :])\n",
    "        return logits\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        real = np.array(y_true) + 1\n",
    "        predict = np.array(y_pred) + 1\n",
    "        percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "        return percentage * 100\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a9f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 300\n",
    "dropout_rate = 0.8\n",
    "future_day = test_data.shape[0]\n",
    "learning_rate = 0.01\n",
    "simulation_size = 10\n",
    "test_size = test_data.shape[0]\n",
    "\n",
    "bilstm = BiLstmModel(\n",
    "    learning_rate, \n",
    "    num_layers, \n",
    "    full_data.shape[1], \n",
    "    size_layer, \n",
    "    full_data.shape[1], \n",
    "    dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc69ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    results.append(forecast(bilstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_accuracy(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbfb0a",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GruModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        super(GruModel, self).__init__()\n",
    "        \n",
    "        self.gru_layers = [\n",
    "            tf.keras.layers.GRU(size_layer, return_sequences=True, dropout=forget_bias)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "    \n",
    "        self.dense = tf.keras.layers.Dense(output_size)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for gru_layer  in self.gru_layer :\n",
    "            x = gru_layer (x)\n",
    "\n",
    "        logits = self.dense(x[:, -1, :])\n",
    "        return logits\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        real = np.array(y_true) + 1\n",
    "        predict = np.array(y_pred) + 1\n",
    "        percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "        return percentage * 100\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1214a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 300\n",
    "dropout_rate = 0.8\n",
    "future_day = test_data.shape[0]\n",
    "learning_rate = 0.01\n",
    "simulation_size = 10\n",
    "test_size = test_data.shape[0]\n",
    "\n",
    "gru = GruModel(\n",
    "    learning_rate, \n",
    "    num_layers, \n",
    "    full_data.shape[1], \n",
    "    size_layer, \n",
    "    full_data.shape[1], \n",
    "    dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fd42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    results.append(forecast(gru))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c343aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_accuracy(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc0521",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b845931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    norm_layer = tf.keras.layers.LayerNormalization(epsilon=epsilon)\n",
    "    return norm_layer(inputs)\n",
    "\n",
    "def multihead_attn(queries, keys, q_masks, k_masks, future_binding, num_units, num_heads):\n",
    "    T_q = tf.shape(queries)[1]                                      \n",
    "    T_k = tf.shape(keys)[1]                  \n",
    "\n",
    "    Q = tf.keras.layers.Dense(num_units, name='Q')(queries)                              \n",
    "    K_V = tf.keras.layers.Dense(2 * num_units, name='K_V')(keys)    \n",
    "    K, V = tf.split(K_V, 2, -1)        \n",
    "\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)                         \n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)                    \n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)                      \n",
    "\n",
    "    align = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))                      \n",
    "    align = align / tf.sqrt(tf.cast(K_.shape[-1], tf.float32))\n",
    "\n",
    "    paddings = tf.fill(tf.shape(align), float('-inf'))                   \n",
    "\n",
    "    key_masks = k_masks                                                 \n",
    "    key_masks = tf.tile(key_masks, [num_heads, 1])                       \n",
    "    key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, T_q, 1])            \n",
    "    align = tf.where(tf.equal(key_masks, 0), paddings, align)       \n",
    "\n",
    "    if future_binding:\n",
    "        lower_tri = tf.ones([T_q, T_k])                                          \n",
    "        lower_tri = tf.linalg.LinearOperatorLowerTriangular(lower_tri).to_dense()  \n",
    "        masks = tf.tile(tf.expand_dims(lower_tri, 0), [tf.shape(align)[0], 1, 1]) \n",
    "        align = tf.where(tf.equal(masks, 0), paddings, align)                      \n",
    "    \n",
    "    align = tf.nn.softmax(align)                                            \n",
    "    query_masks = tf.cast(q_masks, tf.float32)  # Replaced `tf.to_float`\n",
    "    query_masks = tf.tile(query_masks, [num_heads, 1])                             \n",
    "    query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, T_k])            \n",
    "    align *= query_masks\n",
    "    \n",
    "    outputs = tf.matmul(align, V_)                                                 \n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)             \n",
    "    outputs += queries                                                             \n",
    "    outputs = layer_norm(outputs)                                                 \n",
    "    return outputs\n",
    "\n",
    "def pointwise_feedforward(inputs, hidden_units, activation=None):\n",
    "    outputs = tf.keras.layers.Dense(4 * hidden_units, activation=activation)(inputs)\n",
    "    outputs = tf.keras.layers.Dense(hidden_units, activation=None)(outputs)\n",
    "    outputs += inputs\n",
    "    outputs = layer_norm(outputs)\n",
    "    return outputs\n",
    "\n",
    "def sinusoidal_position_encoding(inputs, mask, repr_dim):\n",
    "    T = tf.shape(inputs)[1]\n",
    "    pos = tf.reshape(tf.range(0.0, tf.cast(T, tf.float32), dtype=tf.float32), [-1, 1])\n",
    "    i = np.arange(0, repr_dim, 2, np.float32)\n",
    "    denom = np.reshape(np.power(10000.0, i / repr_dim), [1, -1])\n",
    "    enc = tf.expand_dims(tf.concat([tf.sin(pos / denom), tf.cos(pos / denom)], 1), 0)\n",
    "    return tf.tile(enc, [tf.shape(inputs)[0], 1, 1]) * tf.expand_dims(tf.cast(mask, tf.float32), -1)\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    C = inputs.shape[-1]\n",
    "    return ((1 - epsilon) * inputs) + (epsilon / C)\n",
    "\n",
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, size_layer, embedded_size, learning_rate, size, output_size,\n",
    "                 num_blocks=2, num_heads=8, min_freq=50):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.X = tf.keras.Input(shape=(None, size))  # Replaced placeholder with Keras Input\n",
    "        self.Y = tf.keras.Input(shape=(output_size,))\n",
    "\n",
    "        # Define the encoder embedding\n",
    "        encoder_embedded = tf.keras.layers.Dense(embedded_size)(self.X)\n",
    "        encoder_embedded = tf.keras.layers.Dropout(0.2)(encoder_embedded)\n",
    "        x_mean = tf.reduce_mean(self.X, axis=2)\n",
    "        en_masks = tf.sign(x_mean)\n",
    "        encoder_embedded += sinusoidal_position_encoding(self.X, en_masks, embedded_size)\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            with tf.name_scope(f'encoder_self_attn_{i}'):\n",
    "                encoder_embedded = multihead_attn(queries=encoder_embedded,\n",
    "                                                 keys=encoder_embedded,\n",
    "                                                 q_masks=en_masks,\n",
    "                                                 k_masks=en_masks,\n",
    "                                                 future_binding=False,\n",
    "                                                 num_units=size_layer,\n",
    "                                                 num_heads=num_heads)\n",
    "\n",
    "            with tf.name_scope(f'encoder_feedforward_{i}'):\n",
    "                encoder_embedded = pointwise_feedforward(encoder_embedded,\n",
    "                                                         embedded_size,\n",
    "                                                         activation=tf.nn.relu)\n",
    "                \n",
    "        self.logits = tf.keras.layers.Dense(output_size)(encoder_embedded)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self(inputs)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        real = np.array(y_true) + 1\n",
    "        predict = np.array(y_pred) + 1\n",
    "        percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "        return percentage * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2756279",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 300\n",
    "dropout_rate = 0.8\n",
    "future_day = test_data.shape[0]\n",
    "learning_rate = 0.01\n",
    "simulation_size = 10\n",
    "test_size = test_data.shape[0]\n",
    "\n",
    "attention = Attention(\n",
    "    learning_rate, \n",
    "    num_layers, \n",
    "    full_data.shape[1], \n",
    "    size_layer, \n",
    "    full_data.shape[1], \n",
    "    dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    results.append(forecast(attention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_accuracy(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a0c5f",
   "metadata": {},
   "source": [
    "# Dilated CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def position_encoding(inputs):\n",
    "    T = tf.shape(inputs)[1]\n",
    "    repr_dim = inputs.shape[-1]\n",
    "    pos = tf.reshape(tf.range(0.0, tf.cast(T, tf.float32), dtype=tf.float32), [-1, 1])\n",
    "    i = np.arange(0, repr_dim, 2, np.float32)\n",
    "    denom = np.reshape(np.power(10000.0, i / repr_dim), [1, -1])\n",
    "    enc = tf.expand_dims(tf.concat([tf.sin(pos / denom), tf.cos(pos / denom)], 1), 0)\n",
    "    return tf.tile(enc, [tf.shape(inputs)[0], 1, 1])\n",
    "\n",
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keepdims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "    params_shape = inputs.shape[-1:]\n",
    "    gamma = tf.Variable(tf.ones(params_shape), name='gamma')\n",
    "    beta = tf.Variable(tf.zeros(params_shape), name='beta')\n",
    "    return gamma * normalized + beta\n",
    "\n",
    "def cnn_block(x, dilation_rate, pad_sz, hidden_dim, kernel_size):\n",
    "    x = layer_norm(x)\n",
    "    pad = tf.zeros([tf.shape(x)[0], pad_sz, hidden_dim])\n",
    "    x = tf.keras.layers.Conv1D(filters=hidden_dim,\n",
    "                              kernel_size=kernel_size,\n",
    "                              dilation_rate=dilation_rate,\n",
    "                              padding='valid')(tf.concat([pad, x, pad], 1))\n",
    "    x = x[:, :-pad_sz, :]\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "class CnnModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        kernel_size=3,\n",
    "        n_attn_heads=16,\n",
    "        dropout=0.9,\n",
    "    ):\n",
    "        super(CnnModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.size_layer = size_layer\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_attn_heads = n_attn_heads\n",
    "        self.dropout = dropout\n",
    "        self.dense_encoder = tf.keras.layers.Dense(size_layer)\n",
    "        self.conv_layers = []\n",
    "        self.attn_conv_layers = []\n",
    "        self.attn_dense_h = []\n",
    "        self.attn_dense_g = []\n",
    "        self.attn_dense_zu = []\n",
    "        self.attn_dense_ze = []\n",
    "        self.attn_dense_d = []\n",
    "        self.attn_dense_h_combined = []\n",
    "        self.dense_output = tf.keras.layers.Dense(output_size)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.conv_layers.append(tf.keras.layers.Conv1D(filters=size_layer, kernel_size=kernel_size, dilation_rate=2**i, padding='valid'))\n",
    "            self.attn_conv_layers.append(tf.keras.layers.Conv1D(filters=size_layer, kernel_size=kernel_size, dilation_rate=2**i, padding='valid'))\n",
    "            for j in range(n_attn_heads):\n",
    "                self.attn_dense_h.append(tf.keras.layers.Dense(size_layer // n_attn_heads))\n",
    "                self.attn_dense_g.append(tf.keras.layers.Dense(size_layer // n_attn_heads))\n",
    "                self.attn_dense_zu.append(tf.keras.layers.Dense(size_layer // n_attn_heads))\n",
    "                self.attn_dense_ze.append(tf.keras.layers.Dense(size_layer // n_attn_heads))\n",
    "                self.attn_dense_d.append(tf.keras.layers.Dense(size_layer // n_attn_heads))\n",
    "            self.attn_dense_h_combined.append(tf.keras.layers.Dense(size_layer))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_embedded = self.dense_encoder(inputs)\n",
    "        encoder_embedded += position_encoding(encoder_embedded)\n",
    "\n",
    "        e = tf.identity(encoder_embedded)\n",
    "        for i in range(self.num_layers):\n",
    "            dilation_rate = 2 ** i\n",
    "            pad_sz = (self.kernel_size - 1) * dilation_rate\n",
    "            with tf.name_scope(f'block_{i}'):\n",
    "                encoder_embedded += cnn_block(encoder_embedded, dilation_rate, pad_sz, self.size_layer, self.kernel_size)\n",
    "\n",
    "        encoder_output, output_memory = encoder_embedded, encoder_embedded + e\n",
    "        g = tf.identity(encoder_embedded)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            dilation_rate = 2 ** i\n",
    "            pad_sz = (self.kernel_size - 1) * dilation_rate\n",
    "            with tf.name_scope(f'decode_{i}'):\n",
    "                attn_res = h = cnn_block(encoder_embedded, dilation_rate, pad_sz, self.size_layer, self.kernel_size)\n",
    "\n",
    "            C = []\n",
    "            for j in range(self.n_attn_heads):\n",
    "                h_ = self.attn_dense_h[i * self.n_attn_heads + j](h)\n",
    "                g_ = self.attn_dense_g[i * self.n_attn_heads + j](g)\n",
    "                zu_ = self.attn_dense_zu[i * self.n_attn_heads + j](encoder_output)\n",
    "                ze_ = self.attn_dense_ze[i * self.n_attn_heads + j](output_memory)\n",
    "\n",
    "                d = self.attn_dense_d[i * self.n_attn_heads + j](h_) + g_\n",
    "                dz = tf.matmul(d, tf.transpose(zu_, [0, 2, 1]))\n",
    "                a = tf.nn.softmax(dz)\n",
    "                c_ = tf.matmul(a, ze_)\n",
    "                C.append(c_)\n",
    "\n",
    "            c = tf.concat(C, 2)\n",
    "            h = self.attn_dense_h_combined[i](attn_res + c)\n",
    "            h = tf.nn.dropout(h, rate=1-self.dropout)\n",
    "            encoder_embedded += h\n",
    "\n",
    "        encoder_embedded = tf.sigmoid(encoder_embedded[:, -1, :])\n",
    "        logits = self.dense_output(encoder_embedded)\n",
    "        return logits\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        real = np.array(y_true) + 1\n",
    "        predict = np.array(y_pred) + 1\n",
    "        percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "        return percentage * 100\n",
    "\n",
    "def calculate_accuracy(real, predict):\n",
    "    real = np.array(real) + 1\n",
    "    predict = np.array(predict) + 1\n",
    "    percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "    return percentage * 100\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = test_size\n",
    "epoch = 300\n",
    "dropout_rate = 0.8\n",
    "future_day = test_size\n",
    "learning_rate = 5e-4\n",
    "\n",
    "cnn = CnnModel(\n",
    "        learning_rate, num_layers, full_data.shape[1], size_layer, full_data.shape[1], \n",
    "        dropout = dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb40470",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    results.append(forecast(cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374229b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_accuracy(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
